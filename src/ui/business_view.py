"""
Giao di·ªán ·ª®ng D·ª•ng Th·ª±c T·∫ø (Business App View) - B·∫£ng ƒëi·ªÅu khi·ªÉn cho ph√¢n t√≠ch kinh doanh.
"""
import streamlit as st
import pandas as pd
import numpy as np
import plotly.graph_objects as go
import plotly.express as px
from src.data_layer import get_data_layer
from src.data_preprocessing import DataPreprocessor
from src.algorithms.kmeans import KMeansClustering
from src.algorithms.decision_tree import DecisionTreeCART
from src.algorithms.apriori import AprioriAlgorithm
from src.config import APRIORI_CONFIG, KMEANS_CONFIG, DECISION_TREE_CONFIG


def show_data_analysis():
    """Hi·ªÉn th·ªã t√≠nh nƒÉng Ph√¢n t√≠ch D·ªØ li·ªáu."""
    st.header("üìä Ph√¢n t√≠ch D·ªØ li·ªáu")
    st.write("Th·ªëng k√™ chi ti·∫øt v·ªÅ d·ªØ li·ªáu giao d·ªãch, s·∫£n ph·∫©m v√† kh√°ch h√†ng")
    
    if st.button("üî¨ Ph√¢n t√≠ch D·ªØ li·ªáu", key="data_analysis"):
        with st.spinner("ƒêang ph√¢n t√≠ch d·ªØ li·ªáu..."):
            data_layer = get_data_layer()
            trans_df = data_layer.load_transaction_data(sample_size=None)  # L·∫•y to√†n b·ªô
            product_df = data_layer.load_product_data()
            
            preprocessor = DataPreprocessor()
            analysis = preprocessor.analyze_data(trans_df)
            
            if not analysis:
                st.error("Kh√¥ng th·ªÉ ph√¢n t√≠ch d·ªØ li·ªáu.")
                return
            
            # Hi·ªÉn th·ªã t√≥m t·∫Øt ch√≠nh
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("üìà T·ªïng Records", f"{analysis['total_rows']:,}")
            with col2:
                st.metric("üìã T·ªïng C·ªôt", analysis['total_columns'])
            with col3:
                st.metric("üíæ Dung l∆∞·ª£ng", f"{analysis['memory_usage_mb']:.2f} MB")
            with col4:
                st.metric("üîÑ H√†ng Tr√πng", analysis['duplicate_rows'])
            
            # Chi ti·∫øt t·ª´ng c·ªôt
            st.subheader("üìã Chi ti·∫øt C·ªôt (Columns)")
            col_info_list = []
            for col_name, info in analysis['column_info'].items():
                col_info_list.append({
                    'C·ªôt': col_name,
                    'Ki·ªÉu d·ªØ li·ªáu': info['dtype'],
                    'Unique': f"{info['unique']:,}",
                    'Null': f"{info['null_count']:,}",
                    'Null %': f"{info['null_pct']:.2f}%"
                })
            
            col_df = pd.DataFrame(col_info_list)
            st.dataframe(col_df, use_container_width=True)
            
            st.success("‚úÖ Ph√¢n t√≠ch ho√†n t·∫•t!")


def show_data_preprocessing():
    """Hi·ªÉn th·ªã t√≠nh nƒÉng Ti·ªÅn x·ª≠ l√Ω D·ªØ li·ªáu."""
    st.header("üîß Ti·ªÅn x·ª≠ l√Ω D·ªØ li·ªáu")
    st.write("L√†m s·∫°ch d·ªØ li·ªáu: lo·∫°i b·ªè null, duplicates, v√† c√°c b·∫•t th∆∞·ªùng")
    
    col1, col2 = st.columns(2)
    with col1:
        remove_nulls = st.checkbox("Lo·∫°i b·ªè NULL values", value=True)
    with col2:
        remove_duplicates = st.checkbox("Lo·∫°i b·ªè h√†ng tr√πng", value=True)
    
    if st.button("üîÑ Ti·ªÅn x·ª≠ l√Ω D·ªØ li·ªáu", key="data_preprocessing"):
        with st.spinner("ƒêang ti·ªÅn x·ª≠ l√Ω..."):
            data_layer = get_data_layer()
            trans_df = data_layer.load_transaction_data(sample_size=None)
            
            preprocessor = DataPreprocessor()
            
            # Ph√¢n t√≠ch tr∆∞·ªõc x·ª≠ l√Ω
            st.subheader("üìä Tr∆∞·ªõc X·ª≠ l√Ω")
            col1, col2 = st.columns(2)
            with col1:
                st.info(f"üîπ T·ªïng h√†ng: {len(trans_df):,}")
            with col2:
                st.info(f"üîπ H√†ng Tr√πng: {trans_df.duplicated().sum():,}")
            
            # Ti·ªÅn x·ª≠ l√Ω
            processed_df, preprocessing_info = preprocessor.preprocess_data(
                trans_df, 
                remove_nulls=remove_nulls,
                remove_duplicates=remove_duplicates
            )
            
            # K·∫øt qu·∫£ sau x·ª≠ l√Ω
            st.subheader("üìä Sau X·ª≠ l√Ω")
            col1, col2, col3 = st.columns(3)
            with col1:
                st.success(f"‚úÖ H√†ng c√≤n l·∫°i: {len(processed_df):,}")
            with col2:
                st.info(f"üîπ H√†ng b·ªã lo·∫°i: {preprocessing_info['duplicates_removed'] + preprocessing_info['nulls_removed']:,}")
            with col3:
                st.metric("üìà Data Retention", f"{preprocessing_info['data_retention_pct']:.2f}%")
            
            # Chi ti·∫øt x·ª≠ l√Ω
            st.subheader("üìã Chi ti·∫øt Ti·ªÅn x·ª≠ l√Ω")
            detail_cols = st.columns(2)
            with detail_cols[0]:
                st.write("**H√†ng b·ªã lo·∫°i b·ªè:**")
                st.write(f"- Duplicates: {preprocessing_info['duplicates_removed']:,}")
                st.write(f"- Nulls: {preprocessing_info['nulls_removed']:,}")
            
            with detail_cols[1]:
                st.write("**C·ªôt b·ªã x√≥a (to√†n NULL):**")
                if preprocessing_info['all_null_cols_removed']:
                    for col in preprocessing_info['all_null_cols_removed']:
                        st.write(f"- {col}")
                else:
                    st.write("- Kh√¥ng c√≥ c·ªôt n√†o")
            
            st.success("‚úÖ Ti·ªÅn x·ª≠ l√Ω ho√†n t·∫•t!")


def show_customer_segmentation():
    """Hi·ªÉn th·ªã t√≠nh nƒÉng Ph√¢n kh√∫c Kh√°ch h√†ng."""
    st.header("üë• Ph√¢n kh√∫c Kh√°ch h√†ng")
    st.write("S·ª≠ d·ª•ng k-Means Clustering ƒë·ªÉ chia kh√°ch h√†ng th√†nh c√°c nh√≥m c√≥ t√≠nh ch·∫•t t∆∞∆°ng t·ª±")
    
    n_clusters = st.slider("S·ªë nh√≥m kh√°ch h√†ng", 2, 5, KMEANS_CONFIG['n_clusters'])
    
    if st.button("üîÑ Ph√¢n t√≠ch Kh√°ch h√†ng", key="seg_analyze"):
        with st.spinner("ƒêang ph√¢n t√≠ch..."):
            data_layer = get_data_layer()
            trans_df = data_layer.load_transaction_data()
            
            if len(trans_df) == 0:
                st.error("Kh√¥ng th·ªÉ t·∫£i d·ªØ li·ªáu.")
                return
            
            kmeans = KMeansClustering(n_clusters=n_clusters)
            result = kmeans.fit(trans_df)
            
            if 'error' in result:
                st.error(result['error'])
                return
            
            rfm_data = result['rfm_data']
            
            # Th·ªëng k√™ chung
            st.subheader("üìä Th·ªëng k√™ Kh√°ch h√†ng")
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("T·ªïng kh√°ch h√†ng", len(rfm_data))
            with col2:
                st.metric("Trung b√¨nh Recency", f"{rfm_data['Recency'].mean():.1f} ng√†y")
            with col3:
                st.metric("T·ªïng doanh thu", f"${rfm_data['Monetary'].sum():,.0f}")
            
            # Ph√¢n t√≠ch t·ª´ng nh√≥m
            st.subheader("üéØ Chi ti·∫øt t·ª´ng Nh√≥m")
            
            cluster_strategies = {
                0: "**Chi·∫øn l∆∞·ª£c:** Gi·ªØ ch√¢n - Kh√°ch h√†ng c√≥ gi√° tr·ªã cao, c·∫ßn ch∆∞∆°ng tr√¨nh loyalty",
                1: "**Chi·∫øn l∆∞·ª£c:** K√≠ch ho·∫°t l·∫°i - Kh√°ch h√†ng c≈©, c·∫ßn chi·∫øn d·ªãch re-engagement",
                2: "**Chi·∫øn l∆∞·ª£c:** Ph√°t tri·ªÉn - Kh√°ch h√†ng m·ªõi ho·∫∑c ti·ªÅm nƒÉng, c·∫ßn h·ªó tr·ª£",
                3: "**Chi·∫øn l∆∞·ª£c:** Qu·∫£n l√Ω - Kh√°ch h√†ng trung b√¨nh, t·ªëi ∆∞u h√≥a chi ph√≠",
                4: "**Chi·∫øn l∆∞·ª£c:** Ph·ª•c v·ª• - Kh√°ch h√†ng ƒëa d·∫°ng, c·∫ßn chi·∫øn l∆∞·ª£c ƒëa chi·ªÅu"
            }
            
            for cluster_id in range(n_clusters):
                with st.expander(f"üìå Nh√≥m {cluster_id + 1}", expanded=(cluster_id == 0)):
                    cluster_data = rfm_data[rfm_data['Cluster'] == cluster_id]
                    
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("S·ªë kh√°ch", len(cluster_data))
                    with col2:
                        st.metric("% t·ªïng", f"{len(cluster_data)/len(rfm_data)*100:.1f}%")
                    with col3:
                        st.metric("Avg Spending", f"${cluster_data['Monetary'].mean():.0f}")
                    
                    st.write(cluster_strategies.get(cluster_id, ""))
                    
                    # Chi ti·∫øt RFM
                    rfm_avg = cluster_data[['Recency', 'Frequency', 'Monetary']].mean()
                    st.info(f"""
                    **RFM Profile:**
                    - Recency (ng√†y g·∫ßn ƒë√¢y): {rfm_avg['Recency']:.1f}
                    - Frequency (l·∫ßn mua): {rfm_avg['Frequency']:.1f}
                    - Monetary (chi ti√™u): ${rfm_avg['Monetary']:.0f}
                    """)
            
            # Bi·ªÉu ƒë·ªì ph√¢n b·ªë
            st.subheader("üìà Bi·ªÉu ƒë·ªì Ph√¢n b·ªë")
            
            # Pie chart - Ph√¢n b·ªë kh√°ch h√†ng
            cluster_counts = rfm_data['Cluster'].value_counts().sort_index()
            fig_pie = go.Figure(data=[go.Pie(
                labels=[f"Nh√≥m {i+1}" for i in cluster_counts.index],
                values=cluster_counts.values,
                textposition='inside',
                textinfo='label+percent'
            )])
            fig_pie.update_layout(title="Ph√¢n b·ªë Kh√°ch h√†ng theo Nh√≥m", height=400)
            st.plotly_chart(fig_pie, use_container_width=True)
            
            # Scatter 3D RFM
            fig_3d = px.scatter_3d(rfm_data,
                                   x='Recency', y='Frequency', z='Monetary',
                                   color='Cluster',
                                   title='Ph√¢n b·ªë 3D RFM theo Nh√≥m',
                                   labels={'Cluster': 'Nh√≥m'},
                                   color_discrete_sequence=px.colors.qualitative.Set2)
            st.plotly_chart(fig_3d, use_container_width=True)


def show_campaign_response_prediction():
    """Hi·ªÉn th·ªã t√≠nh nƒÉng D·ª± ƒëo√°n Ph·∫£n h·ªìi Chi·∫øn d·ªãch."""
    st.header("üì¢ D·ª± ƒëo√°n Ph·∫£n h·ªìi Chi·∫øn d·ªãch")
    st.write("S·ª≠ d·ª•ng Decision Tree ƒë·ªÉ d·ª± ƒëo√°n kh√°ch h√†ng n√†o s·∫Ω ph·∫£n h·ªìi chi·∫øn d·ªãch")
    
    if st.button("üîÆ D·ª± ƒëo√°n Ph·∫£n h·ªìi", key="campaign_predict"):
        with st.spinner("ƒêang d·ª± ƒëo√°n..."):
            data_layer = get_data_layer()
            merged = data_layer.get_merged_dataset()
            
            if len(merged) == 0:
                st.error("Kh√¥ng th·ªÉ t·∫£i d·ªØ li·ªáu.")
                return
            
            # T·∫°o nh√£n m·ª•c ti√™u (v√≠ d·ª•: kh√°ch h√†ng mua nhi·ªÅu = ph·∫£n h·ªìi chi·∫øn d·ªãch)
            merged['CAMPAIGN_RESPONSE'] = (merged['SALES_VALUE'] > 
                                          merged['SALES_VALUE'].median()).astype(int)
            
            model = DecisionTreeCART(max_depth=DECISION_TREE_CONFIG['max_depth'], 
                                    min_samples_split=DECISION_TREE_CONFIG['min_samples_split'])
            result = model.train(merged, target_column='CAMPAIGN_RESPONSE')
            
            if 'error' in result:
                st.error(result['error'])
                return
            
            st.success(f"‚úÖ D·ª± ƒëo√°n th√†nh c√¥ng (Accuracy: {result['accuracy']:.2%})")
            
            # Hi·ªÉn th·ªã k·∫øt qu·∫£
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("ƒê·ªô ch√≠nh x√°c", f"{result['accuracy']:.2%}")
            with col2:
                st.metric("S·ªë n√∫t c√¢y", result['num_nodes'])
            with col3:
                st.metric("ƒê·ªô s√¢u c√¢y", result['max_depth'])
            
            # Feature Importance
            st.subheader("üéØ Y·∫øu t·ªë ·∫¢nh h∆∞·ªüng t·ªõi Ph·∫£n h·ªìi Chi·∫øn d·ªãch")
            features_df = pd.DataFrame(list(result['feature_importance'].items()),
                                      columns=['Y·∫øu t·ªë', 'M·ª©c ƒë·ªô ·∫¢nh h∆∞·ªüng'])
            features_df = features_df.sort_values('M·ª©c ƒë·ªô ·∫¢nh h∆∞·ªüng', ascending=True)
            
            fig = go.Figure(data=[
                go.Bar(x=features_df['M·ª©c ƒë·ªô ·∫¢nh h∆∞·ªüng'],
                      y=features_df['Y·∫øu t·ªë'],
                      orientation='h',
                      marker=dict(color='#FF6B6B'))
            ])
            fig.update_layout(
                title="T·∫ßm quan tr·ªçng c√°c Y·∫øu t·ªë",
                xaxis_title="M·ª©c ƒë·ªô ·∫¢nh h∆∞·ªüng",
                height=400
            )
            st.plotly_chart(fig, use_container_width=True)
            
            # Confusion Matrix
            st.subheader("üìä Confusion Matrix - ƒê√°nh gi√° Ch·∫•t l∆∞·ª£ng D·ª± ƒëo√°n")
            cm = result['confusion_matrix']
            fig_cm = go.Figure(data=go.Heatmap(
                z=cm,
                x=['Kh√¥ng ph·∫£n h·ªìi', 'Ph·∫£n h·ªìi'],
                y=['Kh√¥ng ph·∫£n h·ªìi', 'Ph·∫£n h·ªìi'],
                text=cm,
                texttemplate='%{text}',
                colorscale='RdYlGn'
            ))
            fig_cm.update_layout(
                title="Confusion Matrix",
                xaxis_title="D·ª± ƒëo√°n",
                yaxis_title="Th·ª±c t·∫ø",
                height=400
            )
            st.plotly_chart(fig_cm, use_container_width=True)


def show_product_recommendation():
    """Hi·ªÉn th·ªã t√≠nh nƒÉng G·ª£i √Ω S·∫£n ph·∫©m."""
    st.header("üõçÔ∏è G·ª£i √Ω S·∫£n ph·∫©m Th√¥ng minh")
    st.write("S·ª≠ d·ª•ng Association Rules (Apriori) ƒë·ªÉ g·ª£i √Ω s·∫£n ph·∫©m li√™n quan")
    
    # Kh·ªüi t·∫°o session_state cho l∆∞u tr·ªØ rules
    if 'recom_rules_found' not in st.session_state:
        st.session_state.recom_rules_found = False
        st.session_state.apriori_rules = None
        st.session_state.trans_df = None
        st.session_state.product_df = None
    
    if st.button("üîç T√¨m Lu·∫≠t K·∫øt h·ª£p", key="recom_find"):
        with st.spinner("ƒêang x·ª≠ l√Ω..."):
            data_layer = get_data_layer()
            trans_df = data_layer.load_transaction_data()
            product_df = data_layer.load_product_data()
            
            if len(trans_df) == 0 or len(product_df) == 0:
                st.error("Kh√¥ng th·ªÉ t·∫£i d·ªØ li·ªáu.")
                return
            
            apriori = AprioriAlgorithm()
            itemsets, rules = apriori.run(trans_df, min_support=APRIORI_CONFIG['min_support'], 
                                         min_confidence=APRIORI_CONFIG['min_confidence'])
            
            if len(rules) == 0:
                st.warning("Kh√¥ng t√¨m th·∫•y Association Rules.")
                st.session_state.recom_rules_found = False
                return
            
            # L∆∞u v√†o session_state
            st.session_state.apriori_rules = rules
            st.session_state.trans_df = trans_df
            st.session_state.product_df = product_df
            st.session_state.recom_rules_found = True
            
            st.success(f"‚úÖ T√¨m th·∫•y {len(rules)} Lu·∫≠t K·∫øt h·ª£p")
    
    # Hi·ªÉn th·ªã giao di·ªán ch·ªçn s·∫£n ph·∫©m n·∫øu ƒë√£ t√¨m ƒë∆∞·ª£c rules
    if st.session_state.recom_rules_found and st.session_state.apriori_rules is not None:
        rules = st.session_state.apriori_rules
        trans_df = st.session_state.trans_df
        product_df = st.session_state.product_df
        
        apriori = AprioriAlgorithm()
        
        st.divider()
        
        # L·ª±a ch·ªçn s·∫£n ph·∫©m
        st.subheader("üëà Ch·ªçn S·∫£n ph·∫©m")
        
        # L·∫•y c√°c product_id t·ª´ transaction
        product_ids = sorted(trans_df['PRODUCT_ID'].unique())
        selected_product_id = st.selectbox(
            "S·∫£n ph·∫©m g·ªëc:",
            product_ids,
            format_func=lambda x: f"ID: {x}",
            key="product_select_recom"
        )
        
        # T√¨m g·ª£i √Ω
        recommendations = apriori.get_recommendations(selected_product_id, rules)
        
        if len(recommendations) == 0:
            st.info(f"Kh√¥ng c√≥ g·ª£i √Ω cho s·∫£n ph·∫©m ID: {selected_product_id}")
        else:
            st.subheader("üí° S·∫£n ph·∫©m ƒê∆∞·ª£c G·ª£i √Ω")
            
            recom_df = pd.DataFrame({
                'PRODUCT_ID': recommendations
            })
            
            # Merge v·ªõi product info
            if 'PRODUCT_ID' in product_df.columns:
                recom_df = recom_df.merge(product_df, 
                                         on='PRODUCT_ID', 
                                         how='left')
                
                # T·∫°o c·ªôt T√™n S·∫£n ph·∫©m t·ª´ COMMODITY_DESC + SUB_COMMODITY_DESC
                if 'COMMODITY_DESC' in recom_df.columns:
                    recom_df['T√™n S·∫£n ph·∫©m'] = recom_df.apply(
                        lambda row: f"{row['COMMODITY_DESC']} - {row['SUB_COMMODITY_DESC']}" 
                        if 'SUB_COMMODITY_DESC' in recom_df.columns 
                        else row['COMMODITY_DESC'],
                        axis=1
                    )
                
                # Ch·ªçn c·ªôt hi·ªÉn th·ªã
                display_cols = ['PRODUCT_ID', 'T√™n S·∫£n ph·∫©m', 'BRAND', 'DEPARTMENT', 'CURR_SIZE_OF_PRODUCT']
                display_cols = [col for col in display_cols if col in recom_df.columns]
                
            st.dataframe(recom_df[display_cols], use_container_width=True)
            
            # Th·ªëng k√™
            st.info(f"‚úÖ G·ª£i √Ω {len(recommendations)} s·∫£n ph·∫©m li√™n quan")
        
        st.divider()
        
        # Top Association Rules
        st.subheader("üìä Top 10 Lu·∫≠t K·∫øt h·ª£p (Highest Confidence)")
        top_rules = rules.nlargest(10, 'confidence')[
            ['antecedent_str', 'consequent_str', 'support', 'confidence', 'lift']
        ].reset_index(drop=True)
        top_rules.columns = ['S·∫£n ph·∫©m Tr∆∞·ªõc', 'S·∫£n ph·∫©m Sau', 
                            'Support', 'Confidence', 'Lift']
        st.dataframe(top_rules, use_container_width=True)


def show_business_page():
    """Hi·ªÉn th·ªã trang ·ª®ng D·ª•ng Th·ª±c T·∫ø ch√≠nh."""
    st.title("üìä ·ª®ng D·ª•ng Th·ª±c T·∫ø (Business Application)")
    st.write("""
    B·∫£ng ƒëi·ªÅu khi·ªÉn t√≠ch h·ª£p cho ph√¢n t√≠ch kh√°ch h√†ng v√† g·ª£i √Ω s·∫£n ph·∫©m.
    S·ª≠ d·ª•ng c√°c m√¥ h√¨nh Machine Learning ƒë·ªÉ h·ªó tr·ª£ quy·∫øt ƒë·ªãnh kinh doanh.
    """)
    
    feature = st.sidebar.radio(
        "üéØ Ch·ªçn T√≠nh nƒÉng:",
        [
            "ÔøΩ Ph√¢n t√≠ch D·ªØ li·ªáu",
            "üîß Ti·ªÅn x·ª≠ l√Ω D·ªØ li·ªáu",
            "üë• Ph√¢n kh√∫c Kh√°ch h√†ng",
            "üì¢ D·ª± ƒëo√°n Ph·∫£n h·ªìi Chi·∫øn d·ªãch",
            "üõçÔ∏è G·ª£i √Ω S·∫£n ph·∫©m"
        ]
    )
    
    st.sidebar.markdown("---")
    st.sidebar.write("**Quy tr√¨nh:**\n1. üìä Ph√¢n t√≠ch d·ªØ li·ªáu\n2. üîß Ti·ªÅn x·ª≠ l√Ω\n3. ü§ñ Ph√¢n t√≠ch ML")
    
    if "Ph√¢n t√≠ch" in feature and "D·ªØ" in feature:
        show_data_analysis()
    elif "Ti·ªÅn x·ª≠ l√Ω" in feature:
        show_data_preprocessing()
    elif "Ph√¢n kh√∫c" in feature:
        show_customer_segmentation()
    elif "D·ª± ƒëo√°n" in feature:
        show_campaign_response_prediction()
    elif "G·ª£i √Ω" in feature:
        show_product_recommendation()
