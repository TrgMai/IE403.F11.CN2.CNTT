"""
Giao di·ªán Ph√≤ng Th√≠ Nghi·ªám (Lab View) - N∆°i tr·ª±c quan h√≥a c√°c thu·∫≠t to√°n.
"""
import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import plotly.graph_objects as go
import plotly.express as px
from src.data_layer import get_data_layer
from src.algorithms.apriori import AprioriAlgorithm
from src.algorithms.rough_set import RoughSetReduct
from src.algorithms.naive_bayes import NaiveBayesClassifier
from src.algorithms.decision_tree import DecisionTreeCART, DecisionTreeC45, DecisionTreeID3
from src.algorithms.bayesian_network import BayesianNetworkDAG
from src.algorithms.kmeans import KMeansClustering


def show_apriori_lab():
    """Hi·ªÉn th·ªã giao di·ªán Lab cho Apriori Algorithm."""
    st.header("üéØ Thu·∫≠t to√°n Apriori - Khai ph√° Lu·∫≠t K·∫øt h·ª£p")
    
    with st.expander("üìñ Nguy√™n l√Ω ho·∫°t ƒë·ªông", expanded=False):
        st.info("""
        **Apriori** t√¨m c√°c t·∫≠p h·ª£p s·∫£n ph·∫©m (Frequent Itemsets) th∆∞·ªùng ƒë∆∞·ª£c mua c√πng nhau
        v√† sinh ra c√°c Lu·∫≠t K·∫øt h·ª£p (Association Rules).
        
        **Kh√°i ni·ªám ch√≠nh:**
        - **Support**: T·ª∑ l·ªá % giao d·ªãch ch·ª©a itemset
        - **Confidence**: T·ª∑ l·ªá % giao d·ªãch c√≥ B n·∫øu ƒë√£ mua A
        - **Lift**: ƒê·ªô m·∫°nh c·ªßa m·ªëi quan h·ªá (Lift > 1 = quan h·ªá d∆∞∆°ng)
        
        **Qu√° tr√¨nh:**
        1. T√¨m Frequent Itemsets (Support ‚â• min_support)
        2. Sinh ra Association Rules t·ª´ Frequent Itemsets
        3. L·ªçc c√°c Rules c√≥ Confidence ‚â• min_confidence
        """)
    
    col1, col2 = st.columns(2)
    with col1:
        min_support = st.slider(
            "Min Support (%)", 0.1, 10.0, 2.0, 0.1) / 100
    with col2:
        min_confidence = st.slider(
            "Min Confidence (%)", 10, 100, 50, 5) / 100
    
    if st.button("‚ñ∂Ô∏è Ch·∫°y Apriori", key="apriori_run"):
        with st.spinner("ƒêang x·ª≠ l√Ω..."):
            data_layer = get_data_layer()
            trans_df = data_layer.load_transaction_data(sample_size=30000)
            
            if len(trans_df) == 0:
                st.error("Kh√¥ng th·ªÉ t·∫£i d·ªØ li·ªáu giao d·ªãch.")
                return
            
            apriori = AprioriAlgorithm()
            itemsets, rules = apriori.run(trans_df, min_support, min_confidence)
            
            if len(itemsets) == 0:
                st.warning(f"Kh√¥ng t√¨m th·∫•y Frequent Itemsets v·ªõi min_support={min_support:.2%}")
                return
            
            st.success(f"‚úÖ T√¨m th·∫•y {len(itemsets)} Frequent Itemsets")
            
            # Hi·ªÉn th·ªã Frequent Itemsets
            st.subheader("üìä Frequent Itemsets")
            itemsets_display = itemsets.copy()
            itemsets_display['itemsets'] = itemsets_display['itemsets'].apply(
                lambda x: ', '.join(str(i) for i in x)
            )
            st.dataframe(itemsets_display, use_container_width=True)
            
            # Hi·ªÉn th·ªã Association Rules
            if len(rules) > 0:
                st.subheader("üîó Association Rules")
                rules_display = rules[['antecedent_str', 'consequent_str', 
                                       'support', 'confidence', 'lift']].copy()
                rules_display.columns = ['S·∫£n ph·∫©m Tr∆∞·ªõc', 'S·∫£n ph·∫©m Sau', 
                                         'Support', 'Confidence', 'Lift']
                st.dataframe(rules_display.head(20), use_container_width=True)
                
                # Bi·ªÉu ƒë·ªì Support vs Confidence
                fig = go.Figure(data=go.Scatter(
                    x=rules['support'],
                    y=rules['confidence'],
                    mode='markers',
                    marker=dict(
                        size=rules['lift'] * 5,
                        color=rules['lift'],
                        colorscale='Viridis',
                        showscale=True,
                        colorbar=dict(title="Lift")
                    ),
                    text=[f"{row['antecedent_str']} ‚Üí {row['consequent_str']}" 
                          for _, row in rules.iterrows()],
                    hovertemplate="<b>%{text}</b><br>Support: %{x:.3f}<br>Confidence: %{y:.3f}<extra></extra>"
                ))
                fig.update_layout(
                    title="üìà Support vs Confidence (k√≠ch th∆∞·ªõc = Lift)",
                    xaxis_title="Support",
                    yaxis_title="Confidence",
                    height=500,
                    font=dict(size=12)
                )
                st.plotly_chart(fig, use_container_width=True)
            else:
                st.warning("Kh√¥ng t√¨m th·∫•y Association Rules v·ªõi confidence n√†y.")


def show_rough_set_lab():
    """Hi·ªÉn th·ªã giao di·ªán Lab cho Rough Set."""
    st.header("üîç Thu·∫≠t to√°n Rough Set - L·ª±a ch·ªçn ƒê·∫∑c tr∆∞ng")
    
    with st.expander("üìñ Nguy√™n l√Ω ho·∫°t ƒë·ªông", expanded=False):
        st.info("""
        **Rough Set** s·ª≠ d·ª•ng l√Ω thuy·∫øt t·∫≠p h·ª£p ƒë·ªÉ l·ª±a ch·ªçn c√°c ƒë·∫∑c tr∆∞ng quan tr·ªçng
        (Feature Selection) t·ª´ d·ªØ li·ªáu.
        
        **Kh√°i ni·ªám ch√≠nh:**
        - **Reduct**: T·∫≠p h·ª£p t·ªëi thi·ªÉu c√°c ƒë·∫∑c tr∆∞ng v·∫´n gi·ªØ kh·∫£ nƒÉng ph√¢n bi·ªát
        - **Information Gain**: ƒê·ªô gi·∫£m entropy khi s·ª≠ d·ª•ng m·ªôt ƒë·∫∑c tr∆∞ng
        - **Entropy**: ƒê·ªô kh√¥ng ch·∫Øc ch·∫Øn / h·ªón lo·∫°n c·ªßa d·ªØ li·ªáu
        
        **Qu√° tr√¨nh (Greedy):**
        1. T√≠nh Information Gain cho t·ª´ng ƒë·∫∑c tr∆∞ng
        2. Ch·ªçn ƒë·∫∑c tr∆∞ng c√≥ Gain cao nh·∫•t
        3. L·∫∑p l·∫°i cho ƒë·∫øn khi ƒë·∫°t s·ªë l∆∞·ª£ng max ho·∫∑c gain <= 0
        """)
    
    max_features = st.slider("S·ªë ƒë·∫∑c tr∆∞ng t·ªëi ƒëa", 1, 10, 5)
    
    if st.button("‚ñ∂Ô∏è Ch·∫°y Rough Set", key="rough_set_run"):
        with st.spinner("ƒêang x·ª≠ l√Ω..."):
            data_layer = get_data_layer()
            merged = data_layer.get_merged_dataset(sample_size=10000)
            
            if len(merged) == 0:
                st.error("Kh√¥ng th·ªÉ t·∫£i d·ªØ li·ªáu.")
                return
            
            # Ch·ªçn c√°c c·ªôt li√™n quan
            demo_cols = ['AGE_DESC', 'INCOME_DESC', 'MARITAL_STATUS_CODE', 
                        'HOMEOWNER_DESC']
            df_selected = merged[demo_cols].copy()
            
            # T·∫°o c·ªôt m·ª•c ti√™u (chi ti√™u cao/th·∫•p)
            median_spending = merged['SALES_VALUE'].median()
            df_selected['CHI_TIEU_CAO'] = (merged['SALES_VALUE'] > median_spending).astype(int)
            
            rough_set = RoughSetReduct()
            result = rough_set.run(df_selected, target='CHI_TIEU_CAO', 
                                  max_features=max_features)
            
            if 'error' in result:
                st.error(result['error'])
                return
            
            st.success(f"‚úÖ ƒê√£ l·ª±a ch·ªçn {result['num_features_selected']} ƒë·∫∑c tr∆∞ng")
            
            # Hi·ªÉn th·ªã k·∫øt qu·∫£
            st.subheader("üéØ Reduct (ƒê·∫∑c tr∆∞ng ƒë∆∞·ª£c ch·ªçn)")
            st.write(f"**ƒê·∫∑c tr∆∞ng:** {', '.join(result['reduct'])}")
            
            # Bi·ªÉu ƒë·ªì Information Gain
            if result['importance_scores']:
                fig = go.Figure(data=[
                    go.Bar(
                        x=list(result['importance_scores'].keys()),
                        y=list(result['importance_scores'].values()),
                        marker=dict(color='steelblue')
                    )
                ])
                fig.update_layout(
                    title="üìä Information Gain c·ªßa t·ª´ng ƒê·∫∑c tr∆∞ng",
                    xaxis_title="ƒê·∫∑c tr∆∞ng",
                    yaxis_title="Information Gain",
                    height=400
                )
                st.plotly_chart(fig, use_container_width=True)


def show_naive_bayes_lab():
    """Hi·ªÉn th·ªã giao di·ªán Lab cho Na√Øve Bayes."""
    st.header("ü§ñ Thu·∫≠t to√°n Na√Øve Bayes - Ph√¢n l·ªõp X√°c su·∫•t")
    
    with st.expander("üìñ Nguy√™n l√Ω ho·∫°t ƒë·ªông", expanded=False):
        st.info("""
        **Na√Øve Bayes** s·ª≠ d·ª•ng ƒê·ªãnh l√Ω Bayes ƒë·ªÉ d·ª± ƒëo√°n x√°c su·∫•t m·ªôt m·∫´u thu·ªôc v·ªÅ m·ªói l·ªõp.
        
        **C√¥ng th·ª©c Bayes:**
        P(L·ªõp|ƒê·∫∑c tr∆∞ng) = P(ƒê·∫∑c tr∆∞ng|L·ªõp) √ó P(L·ªõp) / P(ƒê·∫∑c tr∆∞ng)
        
        **Gi·∫£ ƒë·ªãnh Na√Øve:** T·∫•t c·∫£ ƒë·∫∑c tr∆∞ng ƒë·ªôc l·∫≠p v·ªõi nhau (kh√¥ng c√≥ m·ªëi li√™n h·ªá).
        
        **Laplace Smoothing:** Th√™m 1 v√†o t·ª≠ s·ªë v√† m·∫´u s·ªë ƒë·ªÉ x·ª≠ l√Ω Zero Probability Problem.
        C√¥ng th·ª©c: P(x|y) = (count(x,y) + 1) / (count(y) + num_classes)
        """)
    
    use_laplace = st.checkbox("‚úì S·ª≠ d·ª•ng Laplace Smoothing", value=True)
    
    if st.button("‚ñ∂Ô∏è Ch·∫°y Na√Øve Bayes", key="naive_bayes_run"):
        with st.spinner("ƒêang x·ª≠ l√Ω..."):
            data_layer = get_data_layer()
            merged = data_layer.get_merged_dataset(sample_size=10000)
            
            if len(merged) == 0:
                st.error("Kh√¥ng th·ªÉ t·∫£i d·ªØ li·ªáu.")
                return
            
            clf = NaiveBayesClassifier(use_laplace_smoothing=use_laplace)
            result = clf.train(merged)
            
            if 'error' in result:
                st.error(result['error'])
                return
            
            st.success(f"‚úÖ Hu·∫•n luy·ªán th√†nh c√¥ng (Accuracy: {result['accuracy']:.2%})")
            
            # Hi·ªÉn th·ªã k·∫øt qu·∫£
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("ƒê·ªô ch√≠nh x√°c (Accuracy)", f"{result['accuracy']:.2%}")
            with col2:
                st.metric("Precision", f"{result['precision']:.2%}")
            with col3:
                st.metric("Recall", f"{result['recall']:.2%}")
            
            # Confusion Matrix
            st.subheader("üìä Confusion Matrix")
            cm = result['confusion_matrix']
            fig = go.Figure(data=go.Heatmap(
                z=cm,
                x=['√Çm t√≠nh', 'D∆∞∆°ng t√≠nh'],
                y=['√Çm t√≠nh', 'D∆∞∆°ng t√≠nh'],
                text=cm,
                texttemplate='%{text}',
                colorscale='Blues'
            ))
            fig.update_layout(
                title="Confusion Matrix",
                xaxis_title="D·ª± ƒëo√°n",
                yaxis_title="Th·ª±c t·∫ø",
                height=400
            )
            st.plotly_chart(fig, use_container_width=True)


def show_decision_tree_lab():
    """Hi·ªÉn th·ªã giao di·ªán Lab cho Decision Tree (CART, C4.5, ID3)."""
    st.header("üå≥ C√¢y Quy·∫øt ƒë·ªãnh (Decision Tree)")
    
    tree_type = st.radio("Ch·ªçn lo·∫°i c√¢y:", 
                         ["CART (Gini Impurity)", "C4.5 (Information Gain)", 
                          "ID3 (Entropy)"])
    
    with st.expander("üìñ Nguy√™n l√Ω ho·∫°t ƒë·ªông", expanded=False):
        if tree_type == "CART (Gini Impurity)":
            st.info("""
            **CART** s·ª≠ d·ª•ng **Gini Impurity** ƒë·ªÉ t√¨m ƒëi·ªÉm ph√¢n chia t·ªët nh·∫•t.
            
            **Gini Impurity:** G = 1 - Œ£(p_i)¬≤
            - Gini = 0: N√∫t thu·∫ßn ch·ªßng (t·∫•t c·∫£ m·ªôt l·ªõp)
            - Gini = 0.5: N√∫t h·ªón h·ª£p (chia ƒë·ªÅu gi·ªØa c√°c l·ªõp)
            """)
        elif tree_type == "C4.5 (Information Gain)":
            st.info("""
            **C4.5 (Quinlan)** s·ª≠ d·ª•ng **Information Gain Ratio**.
            
            **Information Gain:** IG = H(Parent) - Œ£(|Child|/|Parent|) √ó H(Child)
            **Entropy:** H = -Œ£(p_i √ó log‚ÇÇ(p_i))
            - Entropy = 0: N√∫t thu·∫ßn ch·ªßng
            - Entropy = 1: N√∫t h·ªón h·ª£p (2 l·ªõp)
            """)
        else:
            st.info("""
            **ID3** l√† phi√™n b·∫£n ƒë∆°n gi·∫£n c·ªßa C4.5, c≈©ng d√πng **Entropy**.
            Kh√°c bi·ªát: ID3 kh√¥ng c√≥ pruning, d·ªÖ Overfitting tr√™n d·ªØ li·ªáu nh·ªè.
            """)
    
    max_depth = st.slider("ƒê·ªô s√¢u t·ªëi ƒëa (max_depth)", 3, 15, 5)
    min_samples = st.slider("Min samples ƒë·ªÉ t√°ch n√∫t", 2, 20, 10)
    
    if st.button("‚ñ∂Ô∏è Ch·∫°y Decision Tree", key="dt_run"):
        with st.spinner("ƒêang x·ª≠ l√Ω..."):
            data_layer = get_data_layer()
            merged = data_layer.get_merged_dataset(sample_size=10000)
            
            if len(merged) == 0:
                st.error("Kh√¥ng th·ªÉ t·∫£i d·ªØ li·ªáu.")
                return
            
            if tree_type == "CART (Gini Impurity)":
                model = DecisionTreeCART(max_depth=max_depth, 
                                        min_samples_split=min_samples)
            elif tree_type == "C4.5 (Information Gain)":
                model = DecisionTreeC45(max_depth=max_depth,
                                       min_samples_split=min_samples)
            else:
                model = DecisionTreeID3(max_depth=max_depth,
                                       min_samples_split=min_samples)
            
            result = model.train(merged)
            
            if 'error' in result:
                st.error(result['error'])
                return
            
            st.success(f"‚úÖ Hu·∫•n luy·ªán th√†nh c√¥ng (Accuracy: {result['accuracy']:.2%})")
            
            # Th√¥ng tin c√¢y
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("ƒê·ªô ch√≠nh x√°c", f"{result['accuracy']:.2%}")
            with col2:
                st.metric("S·ªë n√∫t", result['num_nodes'])
            with col3:
                st.metric("ƒê·ªô s√¢u", result['max_depth'])
            
            # Feature Importance
            st.subheader("üéØ Feature Importance (T·∫ßm quan tr·ªçng ƒê·∫∑c tr∆∞ng)")
            features_df = pd.DataFrame(list(result['feature_importance'].items()),
                                      columns=['ƒê·∫∑c tr∆∞ng', 'T·∫ßm quan tr·ªçng'])
            features_df = features_df.sort_values('T·∫ßm quan tr·ªçng', ascending=True)
            
            fig = go.Figure(data=[
                go.Bar(x=features_df['T·∫ßm quan tr·ªçng'],
                      y=features_df['ƒê·∫∑c tr∆∞ng'],
                      orientation='h',
                      marker=dict(color='teal'))
            ])
            fig.update_layout(
                title="T·∫ßm quan tr·ªçng c·ªßa t·ª´ng ƒê·∫∑c tr∆∞ng",
                xaxis_title="T·∫ßm quan tr·ªçng",
                height=400
            )
            st.plotly_chart(fig, use_container_width=True)


def show_bayesian_network_lab():
    """Hi·ªÉn th·ªã giao di·ªán Lab cho Bayesian Network."""
    st.header("üï∏Ô∏è Bayesian Network - M√¥ h√¨nh X√°c su·∫•t ƒê·ªì th·ªã")
    
    with st.expander("üìñ Nguy√™n l√Ω ho·∫°t ƒë·ªông", expanded=False):
        st.info("""
        **Bayesian Network** l√† m·ªôt ƒë·ªì th·ªã c√≥ h∆∞·ªõng (DAG) bi·ªÉu di·ªÖn c√°c m·ªëi quan h·ªá
        x√°c su·∫•t gi·ªØa c√°c bi·∫øn.
        
        **C·∫•u tr√∫c DAG:** Tu·ªïi (AGE) ‚Üí Thu nh·∫≠p (INCOME) ‚Üí S·ªü h·ªØu nh√† (HOMEOWNER)
        
        **√ù nghƒ©a:**
        - Tu·ªïi ·∫£nh h∆∞·ªüng ƒë·∫øn Thu nh·∫≠p
        - Thu nh·∫≠p ·∫£nh h∆∞·ªüng ƒë·∫øn S·ªü h·ªØu nh√†
        
        **Suy di·ªÖn (Inference):**
        Cho tr∆∞·ªõc gi√° tr·ªã tu·ªïi, t√≠nh x√°c su·∫•t P(INCOME|AGE) v√† P(HOMEOWNER|AGE).
        """)
    
    if st.button("‚ñ∂Ô∏è Ch·∫°y Bayesian Network", key="bn_run"):
        with st.spinner("ƒêang x·ª≠ l√Ω..."):
            data_layer = get_data_layer()
            merged = data_layer.get_merged_dataset(sample_size=10000)
            
            if len(merged) == 0:
                st.error("Kh√¥ng th·ªÉ t·∫£i d·ªØ li·ªáu.")
                return
            
            bn = BayesianNetworkDAG()
            result = bn.fit(merged)
            
            if 'error' in result:
                st.error(result['error'])
                return
            
            st.success(f"‚úÖ Hu·∫•n luy·ªán th√†nh c√¥ng ({result['num_samples']} m·∫´u)")
            
            # Hi·ªÉn th·ªã c·∫•u tr√∫c DAG
            st.subheader("üìä C·∫•u tr√∫c DAG (Directed Acyclic Graph)")
            col1, col2 = st.columns(2)
            
            with col1:
                st.write("**N√∫t (Nodes):**")
                st.write(", ".join(result['structure']['nodes']))
            
            with col2:
                st.write("**C·∫°nh (Edges):**")
                edges_str = " ‚Üí ".join([f"{e[0]}‚Üí{e[1]}" for e in result['structure']['edges']])
                st.write(edges_str)
            
            # V·∫Ω DAG v·ªõi b·ªë c·ª•c t·ªët h∆°n
            fig = go.Figure()
            
            # T·ªça ƒë·ªô n√∫t v·ªõi b·ªë c·ª•c ngang
            node_positions = {
                'AGE': (0, 2),
                'INCOME': (2, 2),
                'HOMEOWNER': (4, 2)
            }
            
            # V·∫Ω c·∫°nh (edges) v·ªõi m≈©i t√™n
            edges = [('AGE', 'INCOME'), ('INCOME', 'HOMEOWNER')]
            for source, target in edges:
                x0, y0 = node_positions[source]
                x1, y1 = node_positions[target]
                
                # V·∫Ω ƒë∆∞·ªùng
                fig.add_trace(go.Scatter(
                    x=[x0, x1],
                    y=[y0, y1],
                    mode='lines',
                    line=dict(color='rgba(100, 100, 255, 0.5)', width=3),
                    hoverinfo='none',
                    showlegend=False
                ))
                
                # Th√™m m≈©i t√™n (tam gi√°c nh·ªè ·ªü cu·ªëi)
                fig.add_annotation(
                    x=x1, y=y1,
                    ax=x0, ay=y0,
                    xref='x', yref='y',
                    axref='x', ayref='y',
                    arrowhead=3,
                    arrowsize=2,
                    arrowwidth=2,
                    arrowcolor='rgba(100, 100, 255, 0.7)',
                    showarrow=True
                )
            
            # V·∫Ω n√∫t (nodes) v·ªõi m√†u s·∫Øc kh√°c nhau
            node_colors = {'AGE': 'lightcoral', 'INCOME': 'lightyellow', 'HOMEOWNER': 'lightgreen'}
            node_x = [node_positions[node][0] for node in ['AGE', 'INCOME', 'HOMEOWNER']]
            node_y = [node_positions[node][1] for node in ['AGE', 'INCOME', 'HOMEOWNER']]
            node_colors_list = [node_colors[node] for node in ['AGE', 'INCOME', 'HOMEOWNER']]
            
            fig.add_trace(go.Scatter(
                x=node_x,
                y=node_y,
                mode='markers+text',
                marker=dict(
                    size=60,
                    color=node_colors_list,
                    line=dict(color='darkblue', width=3)
                ),
                text=['AGE', 'INCOME', 'HOMEOWNER'],
                textposition='middle center',
                textfont=dict(size=14, color='black', family='Arial Black'),
                hoverinfo='text',
                showlegend=False
            ))
            
            fig.update_layout(
                title="üï∏Ô∏è Bayesian Network DAG: Age ‚Üí Income ‚Üí Homeowner",
                showlegend=False,
                hovermode='closest',
                height=400,
                xaxis=dict(showgrid=False, zeroline=False, showticklabels=False, range=[-0.5, 4.5]),
                yaxis=dict(showgrid=False, zeroline=False, showticklabels=False, range=[0, 3]),
                plot_bgcolor='rgba(240, 240, 250, 0.5)'
            )
            st.plotly_chart(fig, use_container_width=True)
            
            # Hi·ªÉn th·ªã Conditional Probability Tables
            st.subheader("üìã Conditional Probability Distribution (CPD)")
            
            tab1, tab2, tab3 = st.tabs(["P(AGE)", "P(INCOME|AGE)", "P(HOMEOWNER|INCOME)"])
            
            with tab1:
                st.write("**Prior Probability - P(AGE):**")
                if 'cpd_age' in result:
                    cpd_age_df = pd.DataFrame(list(result['cpd_age'].items()), 
                                              columns=['Age Category', 'Probability'])
                    st.dataframe(cpd_age_df, use_container_width=True)
            
            with tab2:
                st.write("**Conditional Probability - P(INCOME|AGE):**")
                if 'cpd_income_given_age' in result:
                    cpd_income = result['cpd_income_given_age']
                    income_data = []
                    for age_key, income_dict in cpd_income.items():
                        for income_key, prob in income_dict.items():
                            income_data.append({
                                'Age Category': age_key,
                                'Income Category': income_key,
                                'Probability': round(prob, 4)
                            })
                    if income_data:
                        cpd_income_df = pd.DataFrame(income_data)
                        st.dataframe(cpd_income_df, use_container_width=True)
                    else:
                        st.info("Kh√¥ng c√≥ d·ªØ li·ªáu CPD Income|Age")
            
            with tab3:
                st.write("**Conditional Probability - P(HOMEOWNER|INCOME):**")
                if 'cpd_homeowner_given_income' in result:
                    cpd_homeowner = result['cpd_homeowner_given_income']
                    homeowner_data = []
                    for income_key, homeowner_dict in cpd_homeowner.items():
                        for homeowner_key, prob in homeowner_dict.items():
                            homeowner_data.append({
                                'Income Category': income_key,
                                'Homeowner Status': homeowner_key,
                                'Probability': round(prob, 4)
                            })
                    if homeowner_data:
                        cpd_homeowner_df = pd.DataFrame(homeowner_data)
                        st.dataframe(cpd_homeowner_df, use_container_width=True)
                    else:
                        st.info("Kh√¥ng c√≥ d·ªØ li·ªáu CPD Homeowner|Income")


def show_kmeans_lab():
    """Hi·ªÉn th·ªã giao di·ªán Lab cho k-Means Clustering."""
    st.header("üé® k-Means Clustering - Ph√¢n nh√≥m RFM")
    
    with st.expander("üìñ Nguy√™n l√Ω ho·∫°t ƒë·ªông", expanded=False):
        st.info("""
        **k-Means** l√† thu·∫≠t to√°n ph√¢n nh√≥m kh√¥ng gi√°m s√°t chia d·ªØ li·ªáu th√†nh k c·ª•m.
        
        **RFM (Recency, Frequency, Monetary):**
        - **Recency:** Ng√†y k·ªÉ t·ª´ l·∫ßn mua cu·ªëi (c√†ng g·∫ßn = c√†ng t·ªët)
        - **Frequency:** S·ªë l·∫ßn mua (c√†ng nhi·ªÅu = c√†ng t·ªët)
        - **Monetary:** T·ªïng chi ti√™u (c√†ng cao = c√†ng t·ªët)
        
        **Qu√° tr√¨nh:**
        1. Kh·ªüi t·∫°o k t√¢m c·ª•m ng·∫´u nhi√™n
        2. G√°n m·ªói ƒëi·ªÉm ƒë·∫øn c·ª•m g·∫ßn nh·∫•t
        3. C·∫≠p nh·∫≠t t√¢m c·ª•m (trung b√¨nh c√°c ƒëi·ªÉm)
        4. L·∫∑p l·∫°i 2-3 cho ƒë·∫øn h·ªôi t·ª•
        
        **Ch·ªâ s·ªë ƒë√°nh gi√°:**
        - **Silhouette Score:** (-1, 1), cao h∆°n = t·ªët h∆°n
        - **Davies-Bouldin Index:** < 1 = t·ªët
        """)
    
    n_clusters = st.slider("S·ªë c·ª•m (k)", 2, 10, 3)
    
    if st.button("‚ñ∂Ô∏è Ch·∫°y k-Means", key="kmeans_run"):
        with st.spinner("ƒêang x·ª≠ l√Ω..."):
            data_layer = get_data_layer()
            trans_df = data_layer.load_transaction_data(sample_size=30000)
            
            if len(trans_df) == 0:
                st.error("Kh√¥ng th·ªÉ t·∫£i d·ªØ li·ªáu giao d·ªãch.")
                return
            
            kmeans = KMeansClustering(n_clusters=n_clusters)
            result = kmeans.fit(trans_df)
            
            if 'error' in result:
                st.error(result['error'])
                return
            
            st.success(f"‚úÖ Ph√¢n nh√≥m th√†nh c√¥ng ({result['n_samples']} kh√°ch h√†ng)")
            
            # Ch·ªâ s·ªë ƒë√°nh gi√°
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("Silhouette Score", f"{result['silhouette_score']:.3f}")
            with col2:
                st.metric("Davies-Bouldin Index", f"{result['davies_bouldin_score']:.3f}")
            with col3:
                st.metric("Inertia", f"{result['inertia']:.0f}")
            
            # Th·ªëng k√™ c·ª•m
            rfm_data = result['rfm_data']
            cluster_stats = kmeans.get_cluster_statistics(rfm_data)
            
            st.subheader("üìä Th·ªëng k√™ C·ª•m")
            for cluster_name, stats in cluster_stats.items():
                st.write(f"**{cluster_name}:**")
                col_info = st.columns(4)
                col_info[0].metric("Kh√°ch h√†ng", f"{stats['S·ªë kh√°ch h√†ng']:.0f}")
                col_info[1].metric("Recency (ng√†y)", f"{stats['Recency trung b√¨nh']:.1f}")
                col_info[2].metric("Frequency", f"{stats['Frequency trung b√¨nh']:.1f}")
                col_info[3].metric("Monetary", f"{stats['Monetary trung b√¨nh']:.0f}")
            
            # Bi·ªÉu ƒë·ªì 3D RFM
            st.subheader("üéØ Ph√¢n b·ªë 3D RFM")
            fig = px.scatter_3d(rfm_data,
                               x='Recency', y='Frequency', z='Monetary',
                               color='Cluster',
                               title='Ph√¢n b·ªë Kh√°ch h√†ng theo RFM',
                               labels={'Cluster': 'C·ª•m'})
            st.plotly_chart(fig, use_container_width=True)


def show_lab_page():
    """Hi·ªÉn th·ªã trang Ph√≤ng Th√≠ Nghi·ªám ch√≠nh."""
    st.title("üß™ Ph√≤ng Th√≠ Nghi·ªám (Academic Lab)")
    st.write("N∆°i tr·ª±c quan h√≥a v√† tinh ch·ªânh 9 thu·∫≠t to√°n Khoa h·ªçc D·ªØ li·ªáu")
    
    algorithm = st.sidebar.radio(
        "üéØ Ch·ªçn Thu·∫≠t to√°n:",
        [
            "1Ô∏è‚É£ Apriori",
            "2Ô∏è‚É£ Rough Set",
            "3Ô∏è‚É£ Na√Øve Bayes",
            "4Ô∏è‚É£ Decision Tree",
            "5Ô∏è‚É£ Bayesian Network",
            "6Ô∏è‚É£ k-Means Clustering"
        ]
    )
    
    st.sidebar.markdown("---")
    st.sidebar.write("**Ghi ch√∫:** C√°c m√¥ h√¨nh s·ª≠ d·ª•ng l·∫•y m·∫´u d·ªØ li·ªáu ƒë·ªÉ t·ªëi ∆∞u hi·ªáu nƒÉng")
    
    if "Apriori" in algorithm:
        show_apriori_lab()
    elif "Rough Set" in algorithm:
        show_rough_set_lab()
    elif "Na√Øve Bayes" in algorithm:
        show_naive_bayes_lab()
    elif "Decision Tree" in algorithm:
        show_decision_tree_lab()
    elif "Bayesian Network" in algorithm:
        show_bayesian_network_lab()
    elif "k-Means" in algorithm:
        show_kmeans_lab()
